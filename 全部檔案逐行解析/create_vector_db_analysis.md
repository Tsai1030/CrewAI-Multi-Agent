# create_vector_db.py é€è¡Œè§£ææ–‡æª”

## æª”æ¡ˆæ¦‚è¿°
é€™æ˜¯å‘é‡è³‡æ–™åº«å»ºç«‹ç¨‹å¼ï¼Œå°ˆé–€ç”¨æ–¼è™•ç†ç´«å¾®æ–—æ•¸PDFæ–‡ä»¶ä¸¦å»ºç«‹æŒä¹…åŒ–å‘é‡åº«ã€‚è©²æª”æ¡ˆå¯¦ç¾äº†å®Œæ•´çš„PDFè™•ç†æµç¨‹ï¼ŒåŒ…æ‹¬å…§å®¹æå–ã€æ™ºèƒ½åˆ†å¡Šã€å…§å®¹åˆ†é¡å’Œå‘é‡åŒ–å­˜å„²ã€‚

## è©³ç´°é€è¡Œè§£æ

### æª”æ¡ˆé ­éƒ¨èˆ‡å°å…¥æ¨¡çµ„ (ç¬¬1-16è¡Œ)

```python
"""
ç°¡å–®çš„å‘é‡è³‡æ–™åº«å»ºç«‹ç¨‹å¼
ä½¿ç”¨ BGE-M3 è™•ç†ç´«å¾®æ–—æ•¸PDFæ–‡ä»¶ï¼Œå»ºç«‹æŒä¹…åŒ–å‘é‡åº«
"""
```
**ç”¨æ„**: æª”æ¡ˆèªªæ˜æ–‡æª”ï¼Œæ˜ç¢ºé€™æ˜¯å°ˆé–€è™•ç†ç´«å¾®æ–—æ•¸PDFçš„å‘é‡åº«å»ºç«‹å·¥å…·

```python
import os
import logging
from pathlib import Path
from typing import List, Dict, Any
import PyPDF2
import chromadb
from chromadb.config import Settings
```
**ç”¨æ„**: å°å…¥å¿…è¦çš„æ¨¡çµ„
- `os`, `pathlib`: æ–‡ä»¶ç³»çµ±æ“ä½œ
- `logging`: æ—¥èªŒè¨˜éŒ„
- `typing`: é¡å‹æç¤º
- `PyPDF2`: PDFæ–‡ä»¶è™•ç†
- `chromadb`: å‘é‡è³‡æ–™åº«

```python
# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
```
**ç”¨æ„**: 
- é…ç½®INFOç´šåˆ¥çš„æ—¥èªŒ
- è¨­ç½®æ™‚é–“æˆ³æ ¼å¼
- å‰µå»ºæ¨¡çµ„å°ˆç”¨æ—¥èªŒè¨˜éŒ„å™¨

### PDFå…§å®¹æå–æ–¹æ³• (ç¬¬18-42è¡Œ)

```python
def extract_pdf_content(pdf_path: str) -> str:
    """æå–PDFå…§å®¹"""
    logger.info(f"é–‹å§‹æå–PDFå…§å®¹: {pdf_path}")
    
    try:
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)
            logger.info(f"PDFç¸½é æ•¸: {total_pages}")
```
**ç”¨æ„**: 
- å®šç¾©PDFå…§å®¹æå–å‡½æ•¸
- ä½¿ç”¨PyPDF2è®€å–PDFæ–‡ä»¶
- è¨˜éŒ„ç¸½é æ•¸ç”¨æ–¼é€²åº¦è¿½è¹¤

```python
            full_text = ""
            for page_num in range(total_pages):
                page = pdf_reader.pages[page_num]
                text = page.extract_text()
                full_text += text + "\n"
                
                if (page_num + 1) % 50 == 0:
                    logger.info(f"å·²è™•ç† {page_num + 1}/{total_pages} é ")
```
**ç”¨æ„**: 
- é€é æå–æ–‡æœ¬å…§å®¹
- æ¯é ä¹‹é–“æ·»åŠ æ›è¡Œç¬¦
- æ¯50é è¨˜éŒ„ä¸€æ¬¡é€²åº¦
- ç´¯ç©æ‰€æœ‰é é¢çš„æ–‡æœ¬

```python
            logger.info(f"PDFå…§å®¹æå–å®Œæˆï¼Œç¸½å­—æ•¸: {len(full_text)}")
            return full_text
            
    except Exception as e:
        logger.error(f"PDFæå–å¤±æ•—: {str(e)}")
        raise
```
**ç”¨æ„**: 
- è¨˜éŒ„æå–å®Œæˆå’Œç¸½å­—æ•¸
- å®Œæ•´çš„ç•°å¸¸è™•ç†
- é‡æ–°æ‹‹å‡ºç•°å¸¸ä¾›ä¸Šå±¤è™•ç†

### å…§å®¹çµæ§‹åˆ†ææ–¹æ³• (ç¬¬44-76è¡Œ)

```python
def analyze_content_structure(text: str) -> Dict[str, Any]:
    """åˆ†æå…§å®¹çµæ§‹"""
    logger.info("åˆ†æPDFå…§å®¹çµæ§‹...")
    
    # æª¢æŸ¥å¸¸è¦‹çš„ç´«å¾®æ–—æ•¸é—œéµè©
    keywords = {
        'ä¸»æ˜Ÿ': ['ç´«å¾®æ˜Ÿ', 'å¤©æ©Ÿæ˜Ÿ', 'å¤ªé™½æ˜Ÿ', 'æ­¦æ›²æ˜Ÿ', 'å¤©åŒæ˜Ÿ', 'å»‰è²æ˜Ÿ', 'å¤©åºœæ˜Ÿ', 'å¤ªé™°æ˜Ÿ', 'è²ªç‹¼æ˜Ÿ', 'å·¨é–€æ˜Ÿ', 'å¤©ç›¸æ˜Ÿ', 'å¤©æ¢æ˜Ÿ', 'ä¸ƒæ®ºæ˜Ÿ', 'ç ´è»æ˜Ÿ'],
        'å®®ä½': ['å‘½å®®', 'å…„å¼Ÿå®®', 'å¤«å¦»å®®', 'å­å¥³å®®', 'è²¡å¸›å®®', 'ç–¾å„å®®', 'é·ç§»å®®', 'å¥´åƒ•å®®', 'å®˜ç¥¿å®®', 'ç”°å®…å®®', 'ç¦å¾·å®®', 'çˆ¶æ¯å®®'],
        'è¼”æ˜Ÿ': ['å·¦è¼”', 'å³å¼¼', 'å¤©é­', 'å¤©é‰', 'æ–‡æ˜Œ', 'æ–‡æ›²', 'ç¥¿å­˜', 'å¤©é¦¬'],
        'ç…æ˜Ÿ': ['æ“ç¾Š', 'é™€ç¾…', 'ç«æ˜Ÿ', 'éˆ´æ˜Ÿ', 'åœ°ç©º', 'åœ°åŠ«'],
        'æ ¼å±€': ['æ ¼å±€', 'ä¸‰åˆ', 'å°å®®', 'æœƒç…§', 'åŒå®®'],
        'é‹å‹¢': ['å¤§é™', 'æµå¹´', 'å°é™', 'é‹å‹¢', 'æµæœˆ', 'æµæ—¥']
    }
```
**ç”¨æ„**: 
- å®šç¾©ç´«å¾®æ–—æ•¸çš„æ ¸å¿ƒé—œéµè©åˆ†é¡
- æ¶µè“‹åå››ä¸»æ˜Ÿã€åäºŒå®®ä½ã€è¼”æ˜Ÿç…æ˜Ÿ
- åŒ…å«æ ¼å±€å’Œé‹å‹¢ç›¸é—œè¡“èª
- ç”¨æ–¼å…§å®¹åˆ†æå’Œåˆ†é¡

```python
    analysis = {
        'total_length': len(text),
        'keyword_counts': {},
        'estimated_sections': 0
    }
    
    # çµ±è¨ˆé—œéµè©å‡ºç¾æ¬¡æ•¸
    for category, words in keywords.items():
        count = sum(text.count(word) for word in words)
        analysis['keyword_counts'][category] = count
        logger.info(f"{category}ç›¸é—œå…§å®¹: {count} æ¬¡æåŠ")
```
**ç”¨æ„**: 
- å‰µå»ºåˆ†æçµæœçµæ§‹
- çµ±è¨ˆæ¯å€‹é¡åˆ¥çš„é—œéµè©å‡ºç¾æ¬¡æ•¸
- è¨˜éŒ„å„é¡åˆ¥çš„å…§å®¹è±å¯Œåº¦
- ç‚ºå¾ŒçºŒåˆ†å¡Šç­–ç•¥æä¾›ä¾æ“š

```python
    # ä¼°ç®—ç« ç¯€æ•¸é‡ï¼ˆåŸºæ–¼å¸¸è¦‹åˆ†éš”ç¬¦ï¼‰
    section_markers = text.count('ç¬¬') + text.count('ç« ') + text.count('ç¯€')
    analysis['estimated_sections'] = section_markers
    
    logger.info(f"ä¼°è¨ˆç« ç¯€æ•¸é‡: {section_markers}")
    
    return analysis
```
**ç”¨æ„**: 
- é€šéå¸¸è¦‹åˆ†éš”ç¬¦ä¼°ç®—æ–‡æª”çµæ§‹
- å¹«åŠ©ç†è§£æ–‡æª”çš„çµ„ç¹”æ–¹å¼
- ç‚ºæ™ºèƒ½åˆ†å¡Šæä¾›çµæ§‹ä¿¡æ¯

### æ™ºèƒ½æ–‡æœ¬åˆ†å¡Šæ–¹æ³• (ç¬¬78-141è¡Œ)

```python
def smart_text_chunking(text: str, analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
    """æ™ºèƒ½æ–‡æœ¬åˆ†å¡Š"""
    logger.info("é–‹å§‹æ™ºèƒ½æ–‡æœ¬åˆ†å¡Š...")
    
    # æ ¹æ“šå…§å®¹åˆ†ææ±ºå®šåˆ†å¡Šç­–ç•¥
    total_length = analysis['total_length']
    
    if total_length < 50000:  # çŸ­æ–‡æª”
        chunk_size = 800
        overlap = 100
    elif total_length < 200000:  # ä¸­ç­‰æ–‡æª”
        chunk_size = 1200
        overlap = 200
    else:  # é•·æ–‡æª”
        chunk_size = 1500
        overlap = 300
```
**ç”¨æ„**: 
- æ ¹æ“šæ–‡æª”é•·åº¦å‹•æ…‹èª¿æ•´åˆ†å¡Šç­–ç•¥
- çŸ­æ–‡æª”ä½¿ç”¨è¼ƒå°çš„å¡Šé¿å…éåº¦åˆ†å‰²
- é•·æ–‡æª”ä½¿ç”¨è¼ƒå¤§çš„å¡Šæé«˜æ•ˆç‡
- è¨­ç½®é©ç•¶çš„é‡ç–Šé¿å…ä¿¡æ¯ä¸Ÿå¤±

```python
    logger.info(f"é¸æ“‡åˆ†å¡Šç­–ç•¥: å¡Šå¤§å°={chunk_size}, é‡ç–Š={overlap}")
    
    chunks = []
    start = 0
    chunk_id = 1
    
    while start < len(text):
        end = start + chunk_size
        
        # å°‹æ‰¾åˆé©çš„åˆ†å‰²é»
        if end < len(text):
            # å„ªå…ˆåœ¨å¥è™Ÿè™•åˆ†å‰²
            last_period = text.rfind('ã€‚', start, end)
            if last_period > start + chunk_size // 2:
                end = last_period + 1
            else:
                # å…¶æ¬¡åœ¨é€—è™Ÿè™•åˆ†å‰²
                last_comma = text.rfind('ï¼Œ', start, end)
                if last_comma > start + chunk_size // 2:
                    end = last_comma + 1
```
**ç”¨æ„**: 
- æ™ºèƒ½å°‹æ‰¾èªç¾©å®Œæ•´çš„åˆ†å‰²é»
- å„ªå…ˆåœ¨å¥è™Ÿè™•åˆ†å‰²ä¿æŒèªç¾©å®Œæ•´
- æ¬¡é¸é€—è™Ÿè™•åˆ†å‰²
- é¿å…åœ¨è©èªä¸­é–“åˆ†å‰²

```python
        chunk_text = text[start:end].strip()
        
        if len(chunk_text) > 50:  # åªä¿ç•™æœ‰æ„ç¾©çš„å¡Š
            # ç°¡å–®çš„å…§å®¹åˆ†é¡
            content_type = classify_content(chunk_text)
            
            chunk_data = {
                'content': chunk_text,
                'metadata': {
                    'chunk_id': chunk_id,
                    'start_pos': start,
                    'end_pos': end,
                    'content_type': content_type,
                    'source': 'ç´«å¾®æ–—æ•°é›†æˆå…¨ä¹¦.pdf'
                }
            }
            chunks.append(chunk_data)
            chunk_id += 1
```
**ç”¨æ„**: 
- éæ¿¾éçŸ­çš„ç„¡æ„ç¾©æ–‡æœ¬å¡Š
- å°æ¯å€‹å¡Šé€²è¡Œå…§å®¹åˆ†é¡
- å‰µå»ºåŒ…å«å…§å®¹å’Œå…ƒæ•¸æ“šçš„çµæ§‹
- è¨˜éŒ„ä½ç½®ä¿¡æ¯ä¾¿æ–¼è¿½è¹¤

### å…§å®¹åˆ†é¡æ–¹æ³• (ç¬¬143-169è¡Œ)

```python
def classify_content(text: str) -> str:
    """ç°¡å–®çš„å…§å®¹åˆ†é¡"""
    text_lower = text.lower()
    
    # ä¸»æ˜Ÿç›¸é—œ
    main_stars = ['ç´«å¾®æ˜Ÿ', 'å¤©æ©Ÿæ˜Ÿ', 'å¤ªé™½æ˜Ÿ', 'æ­¦æ›²æ˜Ÿ', 'å¤©åŒæ˜Ÿ', 'å»‰è²æ˜Ÿ', 'å¤©åºœæ˜Ÿ', 'å¤ªé™°æ˜Ÿ', 'è²ªç‹¼æ˜Ÿ', 'å·¨é–€æ˜Ÿ', 'å¤©ç›¸æ˜Ÿ', 'å¤©æ¢æ˜Ÿ', 'ä¸ƒæ®ºæ˜Ÿ', 'ç ´è»æ˜Ÿ']
    if any(star in text for star in main_stars):
        return 'ä¸»æ˜Ÿè§£æ'
    
    # å®®ä½ç›¸é—œ
    palaces = ['å‘½å®®', 'å…„å¼Ÿå®®', 'å¤«å¦»å®®', 'å­å¥³å®®', 'è²¡å¸›å®®', 'ç–¾å„å®®', 'é·ç§»å®®', 'å¥´åƒ•å®®', 'å®˜ç¥¿å®®', 'ç”°å®…å®®', 'ç¦å¾·å®®', 'çˆ¶æ¯å®®']
    if any(palace in text for palace in palaces):
        return 'å®®ä½è§£æ'
    
    # æ ¼å±€ç›¸é—œ
    if any(word in text for word in ['æ ¼å±€', 'ä¸‰åˆ', 'å°å®®', 'æœƒç…§']):
        return 'æ ¼å±€åˆ†æ'
    
    # é‹å‹¢ç›¸é—œ
    if any(word in text for word in ['å¤§é™', 'æµå¹´', 'é‹å‹¢', 'æµæœˆ']):
        return 'é‹å‹¢åˆ†æ'
    
    # åŸºç¤ç†è«–
    if any(word in text for word in ['åŸºç¤', 'ç†è«–', 'æ¦‚å¿µ', 'åŸç†']):
        return 'åŸºç¤ç†è«–'
    
    return 'ä¸€èˆ¬å…§å®¹'
```
**ç”¨æ„**: 
- æ ¹æ“šé—œéµè©è‡ªå‹•åˆ†é¡æ–‡æœ¬å…§å®¹
- æä¾›å…­ç¨®ä¸»è¦å…§å®¹é¡å‹
- å„ªå…ˆç´šé †åºï¼šä¸»æ˜Ÿâ†’å®®ä½â†’æ ¼å±€â†’é‹å‹¢â†’ç†è«–â†’ä¸€èˆ¬
- ä¾¿æ–¼å¾ŒçºŒçš„å°ˆæ¥­åŒ–æª¢ç´¢

## ç¨‹å¼ç¢¼æ¶æ§‹ç¸½çµ

### è¨­è¨ˆæ¨¡å¼
1. **ç®¡é“æ¨¡å¼**: PDFæå–â†’å…§å®¹åˆ†æâ†’æ™ºèƒ½åˆ†å¡Šâ†’å‘é‡åŒ–å­˜å„²
2. **ç­–ç•¥æ¨¡å¼**: æ ¹æ“šæ–‡æª”å¤§å°é¸æ“‡ä¸åŒçš„åˆ†å¡Šç­–ç•¥
3. **åˆ†é¡æ¨¡å¼**: åŸºæ–¼é—œéµè©çš„å…§å®¹è‡ªå‹•åˆ†é¡
4. **æ‰¹æ¬¡è™•ç†**: åˆ†æ‰¹è™•ç†å¤§é‡æ–‡æª”æé«˜æ•ˆç‡

### ä¸»è¦ç‰¹é»
- **æ™ºèƒ½åˆ†å¡Š**: æ ¹æ“šæ–‡æª”å¤§å°å’Œèªç¾©é‚Šç•Œé€²è¡Œåˆ†å¡Š
- **å…§å®¹åˆ†æ**: å°ˆæ¥­çš„ç´«å¾®æ–—æ•¸é—œéµè©åˆ†æ
- **è‡ªå‹•åˆ†é¡**: åŸºæ–¼å…§å®¹çš„æ™ºèƒ½åˆ†é¡ç³»çµ±
- **é€²åº¦è¿½è¹¤**: è©³ç´°çš„è™•ç†é€²åº¦å’Œçµ±è¨ˆä¿¡æ¯

### å‘é‡è³‡æ–™åº«å»ºç«‹æ–¹æ³• (ç¬¬171-258è¡Œ)

```python
def create_vector_database(chunks: List[Dict[str, Any]], db_name: str = "test1"):
    """å»ºç«‹å‘é‡è³‡æ–™åº«"""
    logger.info(f"é–‹å§‹å»ºç«‹å‘é‡è³‡æ–™åº«: {db_name}")

    try:
        # è¨­ç½® BGE-M3 åµŒå…¥
        from src.rag.bge_embeddings import BGEM3Embeddings

        # å‰µå»ºåµŒå…¥æ¨¡å‹
        embeddings = BGEM3Embeddings(
            model_name="BAAI/bge-m3",
            device="cpu",
            max_length=1024,
            batch_size=8,
            use_fp16=False
        )

        logger.info("BGE-M3 åµŒå…¥æ¨¡å‹è¼‰å…¥æˆåŠŸ")
```
**ç”¨æ„**:
- å®šç¾©å‘é‡è³‡æ–™åº«å»ºç«‹å‡½æ•¸
- å°å…¥è‡ªå®šç¾©çš„BGE-M3åµŒå…¥æ¨¡å‹
- é…ç½®CPUé‹è¡Œå’Œè¼ƒå°çš„æ‰¹æ¬¡å¤§å°
- ä½¿ç”¨1024æœ€å¤§é•·åº¦é©åˆä¸­æ–‡æ–‡æœ¬

```python
        # å‰µå»º ChromaDB å®¢æˆ¶ç«¯
        persist_directory = f"./vector_db_{db_name}"
        client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )

        # å‰µå»ºæˆ–ç²å–é›†åˆ
        collection_name = f"ziwei_knowledge_{db_name}"
        try:
            collection = client.get_collection(collection_name)
            logger.info(f"ä½¿ç”¨ç¾æœ‰é›†åˆ: {collection_name}")
        except:
            collection = client.create_collection(collection_name)
            logger.info(f"å‰µå»ºæ–°é›†åˆ: {collection_name}")
```
**ç”¨æ„**:
- å‰µå»ºæŒä¹…åŒ–çš„ChromaDBå®¢æˆ¶ç«¯
- ä½¿ç”¨å‹•æ…‹çš„è³‡æ–™åº«åç¨±æ”¯æ´å¤šå€‹ç‰ˆæœ¬
- å˜—è©¦è¼‰å…¥ç¾æœ‰é›†åˆï¼Œä¸å­˜åœ¨å‰‡å‰µå»ºæ–°çš„
- é—œé–‰åŒ¿åé™æ¸¬ä¿è­·éš±ç§

```python
        # æ‰¹æ¬¡è™•ç†æ–‡æª”
        batch_size = 50
        total_chunks = len(chunks)

        for i in range(0, total_chunks, batch_size):
            batch_chunks = chunks[i:i + batch_size]

            # æº–å‚™æ‰¹æ¬¡æ•¸æ“š
            texts = [chunk['content'] for chunk in batch_chunks]
            metadatas = [chunk['metadata'] for chunk in batch_chunks]
            ids = [f"chunk_{chunk['metadata']['chunk_id']}" for chunk in batch_chunks]

            # ç”ŸæˆåµŒå…¥
            logger.info(f"è™•ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(total_chunks + batch_size - 1)//batch_size}")
            embeddings_vectors = embeddings.embed_documents(texts)

            # æ·»åŠ åˆ°å‘é‡åº«
            collection.add(
                embeddings=embeddings_vectors,
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )

            logger.info(f"å·²æ·»åŠ  {len(batch_chunks)} å€‹æ–‡æª”åˆ°å‘é‡åº«")
```
**ç”¨æ„**:
- ä½¿ç”¨50å€‹æ–‡æª”çš„æ‰¹æ¬¡å¤§å°å¹³è¡¡æ•ˆç‡å’Œå…§å­˜
- åˆ†åˆ¥æå–æ–‡æœ¬ã€å…ƒæ•¸æ“šå’ŒID
- ç”Ÿæˆå”¯ä¸€çš„æ–‡æª”ID
- æ‰¹æ¬¡ç”ŸæˆåµŒå…¥å‘é‡æé«˜æ•ˆç‡
- è¨˜éŒ„è©³ç´°çš„è™•ç†é€²åº¦

```python
        # æª¢æŸ¥æœ€çµ‚ç‹€æ…‹
        final_count = collection.count()
        logger.info(f"å‘é‡è³‡æ–™åº«å»ºç«‹å®Œæˆï¼")
        logger.info(f"è³‡æ–™åº«åç¨±: {db_name}")
        logger.info(f"å­˜å„²è·¯å¾‘: {persist_directory}")
        logger.info(f"é›†åˆåç¨±: {collection_name}")
        logger.info(f"ç¸½æ–‡æª”æ•¸: {final_count}")

        # æ¸¬è©¦æœç´¢
        logger.info("æ¸¬è©¦æœç´¢åŠŸèƒ½...")
        test_query = "ç´«å¾®æ˜Ÿçš„ç‰¹è³ª"
        query_embedding = embeddings.embed_query(test_query)

        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=3
        )

        logger.info(f"æœç´¢æ¸¬è©¦æˆåŠŸï¼Œæ‰¾åˆ° {len(results['documents'][0])} æ¢ç›¸é—œçµæœ")

        return True
```
**ç”¨æ„**:
- é©—è­‰æœ€çµ‚çš„æ–‡æª”æ•¸é‡
- è¨˜éŒ„å®Œæ•´çš„è³‡æ–™åº«ä¿¡æ¯
- åŸ·è¡Œæœç´¢æ¸¬è©¦ç¢ºä¿åŠŸèƒ½æ­£å¸¸
- ä½¿ç”¨ç´«å¾®æ–—æ•¸ç›¸é—œæŸ¥è©¢é€²è¡Œæ¸¬è©¦
- è¿”å›æˆåŠŸç‹€æ…‹

### ä¸»ç¨‹å¼å…¥å£ (ç¬¬260-302è¡Œ)

```python
def main():
    """ä¸»å‡½æ•¸"""
    pdf_path = r"C:\Users\user\Desktop\test2\ç´«å¾®æ–—æ•°é›†æˆå…¨ä¹¦.pdf"
    db_name = "test1"

    print("ğŸŒŸ ç´«å¾®æ–—æ•¸å‘é‡è³‡æ–™åº«å»ºç«‹ç¨‹å¼")
    print(f"ğŸ“ PDFæ–‡ä»¶: {pdf_path}")
    print(f"ğŸ—„ï¸ è³‡æ–™åº«åç¨±: {db_name}")
    print("=" * 60)
```
**ç”¨æ„**:
- å®šç¾©ä¸»ç¨‹å¼å…¥å£å‡½æ•¸
- è¨­ç½®PDFæ–‡ä»¶è·¯å¾‘å’Œè³‡æ–™åº«åç¨±
- ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿå¢å¼·ç”¨æˆ¶é«”é©—
- é¡¯ç¤ºæ¸…æ™°çš„ç¨‹å¼ä¿¡æ¯

```python
    try:
        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(pdf_path).exists():
            logger.error(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
            return

        # 1. æå–PDFå…§å®¹
        text = extract_pdf_content(pdf_path)

        # 2. åˆ†æå…§å®¹çµæ§‹
        analysis = analyze_content_structure(text)

        # 3. æ™ºèƒ½åˆ†å¡Š
        chunks = smart_text_chunking(text, analysis)

        # 4. å»ºç«‹å‘é‡è³‡æ–™åº«
        success = create_vector_database(chunks, db_name)
```
**ç”¨æ„**:
- æª¢æŸ¥PDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨
- åŸ·è¡Œå››æ­¥é©Ÿè™•ç†æµç¨‹
- æ¯å€‹æ­¥é©Ÿéƒ½æœ‰æ¸…æ™°çš„ç·¨è™Ÿå’Œèªªæ˜
- æŒ‰é †åºåŸ·è¡Œç¢ºä¿æ•¸æ“šæµçš„æ­£ç¢ºæ€§

```python
        if success:
            print("\nğŸ‰ å‘é‡è³‡æ–™åº«å»ºç«‹æˆåŠŸï¼")
            print(f"ğŸ“ ä½ç½®: ./vector_db_{db_name}")
            print(f"ğŸ“Š æ–‡æª”æ•¸: {len(chunks)}")
            print("\nâœ… å¯ä»¥é–‹å§‹ä½¿ç”¨å‘é‡è³‡æ–™åº«é€²è¡Œæœç´¢äº†ï¼")
        else:
            print("\nâŒ å‘é‡è³‡æ–™åº«å»ºç«‹å¤±æ•—")

    except Exception as e:
        logger.error(f"ç¨‹å¼åŸ·è¡Œå¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
```
**ç”¨æ„**:
- æ ¹æ“šåŸ·è¡Œçµæœé¡¯ç¤ºç›¸æ‡‰çš„æˆåŠŸæˆ–å¤±æ•—ä¿¡æ¯
- æä¾›è³‡æ–™åº«ä½ç½®å’Œçµ±è¨ˆä¿¡æ¯
- å®Œæ•´çš„ç•°å¸¸è™•ç†å’ŒéŒ¯èª¤è¿½è¹¤
- æ¨™æº–çš„ç¨‹å¼å…¥å£æª¢æŸ¥

## æ·±åº¦æŠ€è¡“åˆ†æ

### æ™ºèƒ½åˆ†å¡Šç­–ç•¥

#### 1. å‹•æ…‹åˆ†å¡Šå¤§å°
```python
if total_length < 50000:  # çŸ­æ–‡æª”
    chunk_size = 800
elif total_length < 200000:  # ä¸­ç­‰æ–‡æª”
    chunk_size = 1200
else:  # é•·æ–‡æª”
    chunk_size = 1500
```
- **çŸ­æ–‡æª”**: ä½¿ç”¨è¼ƒå°å¡Šé¿å…éåº¦åˆ†å‰²
- **ä¸­ç­‰æ–‡æª”**: å¹³è¡¡å¡Šå¤§å°å’Œæª¢ç´¢ç²¾åº¦
- **é•·æ–‡æª”**: ä½¿ç”¨è¼ƒå¤§å¡Šæé«˜è™•ç†æ•ˆç‡

#### 2. èªç¾©é‚Šç•Œåˆ†å‰²
```python
last_period = text.rfind('ã€‚', start, end)
if last_period > start + chunk_size // 2:
    end = last_period + 1
```
- å„ªå…ˆåœ¨å¥è™Ÿè™•åˆ†å‰²ä¿æŒèªç¾©å®Œæ•´
- è¨­ç½®æœ€å°å¡Šå¤§å°é¿å…éå°çš„ç‰‡æ®µ
- æ¬¡é¸é€—è™Ÿè™•åˆ†å‰²ä½œç‚ºå‚™é¸æ–¹æ¡ˆ

#### 3. é‡ç–Šç­–ç•¥
- æ ¹æ“šå¡Šå¤§å°è¨­ç½®ç›¸æ‡‰çš„é‡ç–Šé•·åº¦
- é¿å…é‡è¦ä¿¡æ¯åœ¨åˆ†å‰²é‚Šç•Œä¸Ÿå¤±
- æé«˜æª¢ç´¢çš„å¬å›ç‡

### å°ˆæ¥­åŒ–å…§å®¹è™•ç†

#### 1. ç´«å¾®æ–—æ•¸é—œéµè©é«”ç³»
- **åå››ä¸»æ˜Ÿ**: å®Œæ•´çš„ä¸»æ˜Ÿåˆ—è¡¨
- **åäºŒå®®ä½**: æ¨™æº–çš„å®®ä½é«”ç³»
- **è¼”æ˜Ÿç…æ˜Ÿ**: é‡è¦çš„è¼”åŠ©æ˜Ÿæ›œ
- **æ ¼å±€é‹å‹¢**: åˆ†æç›¸é—œè¡“èª

#### 2. å…§å®¹åˆ†é¡ç³»çµ±
- åŸºæ–¼é—œéµè©çš„è‡ªå‹•åˆ†é¡
- å…­ç¨®ä¸»è¦å…§å®¹é¡å‹
- å„ªå…ˆç´šæ’åºç¢ºä¿æº–ç¢ºåˆ†é¡

### å‘é‡åŒ–è™•ç†å„ªåŒ–

#### 1. BGE-M3é…ç½®å„ªåŒ–
```python
embeddings = BGEM3Embeddings(
    model_name="BAAI/bge-m3",
    device="cpu",
    max_length=1024,
    batch_size=8,
    use_fp16=False
)
```
- ä½¿ç”¨CPUé‹è¡Œé©åˆä¸€èˆ¬ç¡¬ä»¶
- 1024é•·åº¦é©åˆä¸­æ–‡æ–‡æœ¬
- è¼ƒå°æ‰¹æ¬¡å¤§å°é¿å…å…§å­˜å•é¡Œ

#### 2. æ‰¹æ¬¡è™•ç†ç­–ç•¥
- 50å€‹æ–‡æª”çš„æ‰¹æ¬¡å¤§å°
- å¹³è¡¡è™•ç†æ•ˆç‡å’Œå…§å­˜ä½¿ç”¨
- è©³ç´°çš„é€²åº¦è¿½è¹¤

### å“è³ªä¿è­‰æ©Ÿåˆ¶

#### 1. å¤šå±¤é©—è­‰
- PDFæ–‡ä»¶å­˜åœ¨æ€§æª¢æŸ¥
- å…§å®¹æå–æˆåŠŸé©—è­‰
- å‘é‡åº«å»ºç«‹ç‹€æ…‹æª¢æŸ¥
- æœç´¢åŠŸèƒ½æ¸¬è©¦

#### 2. éŒ¯èª¤è™•ç†
- æ¯å€‹æ­¥é©Ÿçš„ç•°å¸¸æ•ç²
- è©³ç´°çš„éŒ¯èª¤æ—¥èªŒ
- å®Œæ•´çš„å †æ£§è¿½è¹¤

## ä½¿ç”¨å ´æ™¯

### 1. çŸ¥è­˜åº«å»ºè¨­
- è™•ç†å°ˆæ¥­é ˜åŸŸPDFæ–‡æª”
- å»ºç«‹æŒä¹…åŒ–å‘é‡è³‡æ–™åº«
- æ”¯æ´èªç¾©æœç´¢å’Œæª¢ç´¢

### 2. å…§å®¹åˆ†æ
- è‡ªå‹•åˆ†ææ–‡æª”çµæ§‹
- çµ±è¨ˆé—œéµè©åˆ†ä½ˆ
- è©•ä¼°å…§å®¹è±å¯Œåº¦

### 3. ç³»çµ±åˆå§‹åŒ–
- ç‚ºRAGç³»çµ±æä¾›çŸ¥è­˜åŸºç¤
- ä¸€æ¬¡æ€§å»ºç«‹ï¼Œå¤šæ¬¡ä½¿ç”¨
- æ”¯æ´ä¸åŒç‰ˆæœ¬çš„è³‡æ–™åº«

## ç¸½çµ

create_vector_db.pyå¯¦ç¾äº†å®Œæ•´çš„PDFåˆ°å‘é‡è³‡æ–™åº«çš„è½‰æ›æµç¨‹ï¼Œé€šéæ™ºèƒ½åˆ†å¡Šã€å°ˆæ¥­åˆ†é¡å’Œé«˜æ•ˆå‘é‡åŒ–ï¼Œç‚ºç´«å¾®æ–—æ•¸AIç³»çµ±æä¾›äº†é«˜è³ªé‡çš„çŸ¥è­˜åŸºç¤ã€‚å…¶æ¨¡çµ„åŒ–è¨­è¨ˆã€æ™ºèƒ½è™•ç†ç­–ç•¥å’Œå®Œæ•´çš„å“è³ªä¿è­‰æ©Ÿåˆ¶ï¼Œä½¿å…¶æˆç‚ºæ§‹å»ºå°ˆæ¥­é ˜åŸŸRAGç³»çµ±çš„å„ªç§€å·¥å…·ã€‚
