"""
PDF æ–‡ä»¶å°å…¥å‘é‡è³‡æ–™åº«å·¥å…·
å°ˆé–€ç”¨æ–¼è™•ç†ç´«å¾®æ–—æ•¸PDFæ–‡ä»¶ä¸¦å»ºç«‹æŒä¹…åŒ–å‘é‡åº«
"""

import asyncio
import logging
import json
import os
from pathlib import Path
from typing import List, Dict, Any
import argparse

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def extract_text_from_pdf(pdf_path: str) -> str:
    """å¾PDFæ–‡ä»¶æå–æ–‡æœ¬"""
    try:
        # å˜—è©¦ä½¿ç”¨ PyPDF2
        try:
            import PyPDF2
            
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                
                logger.info(f"PDF ç¸½é æ•¸: {len(pdf_reader.pages)}")
                
                for page_num, page in enumerate(pdf_reader.pages, 1):
                    page_text = page.extract_text()
                    text += f"\n--- ç¬¬ {page_num} é  ---\n{page_text}\n"
                    
                    if page_num % 10 == 0:
                        logger.info(f"å·²è™•ç† {page_num} é ...")
                
                logger.info(f"PDF æ–‡æœ¬æå–å®Œæˆï¼Œç¸½å­—æ•¸: {len(text)}")
                return text
                
        except ImportError:
            logger.warning("PyPDF2 æœªå®‰è£ï¼Œå˜—è©¦ä½¿ç”¨ pdfplumber...")
            
            # å˜—è©¦ä½¿ç”¨ pdfplumber
            try:
                import pdfplumber
                
                text = ""
                with pdfplumber.open(pdf_path) as pdf:
                    logger.info(f"PDF ç¸½é æ•¸: {len(pdf.pages)}")
                    
                    for page_num, page in enumerate(pdf.pages, 1):
                        page_text = page.extract_text()
                        if page_text:
                            text += f"\n--- ç¬¬ {page_num} é  ---\n{page_text}\n"
                        
                        if page_num % 10 == 0:
                            logger.info(f"å·²è™•ç† {page_num} é ...")
                
                logger.info(f"PDF æ–‡æœ¬æå–å®Œæˆï¼Œç¸½å­—æ•¸: {len(text)}")
                return text
                
            except ImportError:
                logger.error("è«‹å®‰è£ PDF è™•ç†åº«: pip install PyPDF2 æˆ– pip install pdfplumber")
                return None
                
    except Exception as e:
        logger.error(f"PDF æ–‡æœ¬æå–å¤±æ•—: {str(e)}")
        return None

def split_text_into_chunks(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """å°‡é•·æ–‡æœ¬åˆ†å‰²æˆé©åˆçš„å¡Š"""
    if not text:
        return []
    
    chunks = []
    start = 0
    text_length = len(text)
    
    while start < text_length:
        end = start + chunk_size
        
        # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€å¡Šï¼Œå˜—è©¦åœ¨å¥è™Ÿè™•åˆ†å‰²
        if end < text_length:
            # å°‹æ‰¾æœ€è¿‘çš„å¥è™Ÿ
            last_period = text.rfind('ã€‚', start, end)
            if last_period > start:
                end = last_period + 1
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        start = end - overlap if end < text_length else end
    
    logger.info(f"æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…± {len(chunks)} å€‹å¡Š")
    return chunks

def create_knowledge_from_chunks(chunks: List[str], source_file: str) -> List[Dict[str, Any]]:
    """å°‡æ–‡æœ¬å¡Šè½‰æ›ç‚ºçŸ¥è­˜æ ¼å¼"""
    knowledge_items = []
    
    for i, chunk in enumerate(chunks, 1):
        # å˜—è©¦è­˜åˆ¥å…§å®¹é¡å‹
        content_type = "general"
        category = "ç´«å¾®æ–—æ•¸"
        
        # ç°¡å–®çš„å…§å®¹åˆ†é¡
        if any(star in chunk for star in ['ç´«å¾®æ˜Ÿ', 'å¤©æ©Ÿæ˜Ÿ', 'å¤ªé™½æ˜Ÿ', 'æ­¦æ›²æ˜Ÿ', 'å¤©åŒæ˜Ÿ', 'å»‰è²æ˜Ÿ']):
            content_type = "ä¸»æ˜Ÿè§£æ"
        elif any(palace in chunk for palace in ['å‘½å®®', 'å¤«å¦»å®®', 'è²¡å¸›å®®', 'äº‹æ¥­å®®']):
            content_type = "å®®ä½è§£æ"
        elif any(concept in chunk for concept in ['æ ¼å±€', 'çµ„åˆ', 'æœƒç…§']):
            content_type = "æ ¼å±€åˆ†æ"
        elif any(fortune in chunk for fortune in ['é‹å‹¢', 'æµå¹´', 'å¤§é™']):
            content_type = "é‹å‹¢åˆ†æ"
        
        knowledge_item = {
            "content": chunk,
            "metadata": {
                "source": source_file,
                "chunk_id": i,
                "category": category,
                "content_type": content_type,
                "total_chunks": len(chunks)
            }
        }
        
        knowledge_items.append(knowledge_item)
    
    return knowledge_items

async def import_pdf_to_vector_db(pdf_path: str, chunk_size: int = 1000, overlap: int = 200):
    """å°‡PDFæ–‡ä»¶å°å…¥å‘é‡è³‡æ–™åº«"""
    
    print(f"ğŸŒŸ PDF å°å…¥å‘é‡è³‡æ–™åº«å·¥å…·")
    print(f"ğŸ“ PDF æ–‡ä»¶: {pdf_path}")
    print("=" * 60)
    
    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(pdf_path).exists():
        logger.error(f"PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return False
    
    try:
        # 1. æå–PDFæ–‡æœ¬
        print("ğŸ“– æ­¥é©Ÿ 1: æå–PDFæ–‡æœ¬...")
        text = extract_text_from_pdf(pdf_path)
        
        if not text:
            logger.error("PDF æ–‡æœ¬æå–å¤±æ•—")
            return False
        
        print(f"âœ… æ–‡æœ¬æå–æˆåŠŸï¼Œç¸½å­—æ•¸: {len(text)}")
        
        # 2. åˆ†å‰²æ–‡æœ¬
        print(f"âœ‚ï¸  æ­¥é©Ÿ 2: åˆ†å‰²æ–‡æœ¬ (å¡Šå¤§å°: {chunk_size}, é‡ç–Š: {overlap})...")
        chunks = split_text_into_chunks(text, chunk_size, overlap)
        
        if not chunks:
            logger.error("æ–‡æœ¬åˆ†å‰²å¤±æ•—")
            return False
        
        print(f"âœ… æ–‡æœ¬åˆ†å‰²æˆåŠŸï¼Œå…± {len(chunks)} å€‹å¡Š")
        
        # 3. å‰µå»ºçŸ¥è­˜æ ¼å¼
        print("ğŸ“ æ­¥é©Ÿ 3: è½‰æ›ç‚ºçŸ¥è­˜æ ¼å¼...")
        source_filename = Path(pdf_path).name
        knowledge_items = create_knowledge_from_chunks(chunks, source_filename)
        
        print(f"âœ… çŸ¥è­˜æ ¼å¼è½‰æ›æˆåŠŸï¼Œå…± {len(knowledge_items)} æ¢çŸ¥è­˜")
        
        # 4. ä¿å­˜ç‚ºJSONæ–‡ä»¶ï¼ˆå¯é¸ï¼‰
        output_json = f"data/knowledge/{Path(pdf_path).stem}_knowledge.json"
        os.makedirs("data/knowledge", exist_ok=True)
        
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(knowledge_items, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… çŸ¥è­˜æ–‡ä»¶å·²ä¿å­˜: {output_json}")
        
        # 5. å°å…¥å‘é‡è³‡æ–™åº«
        print("ğŸ—„ï¸  æ­¥é©Ÿ 4: å°å…¥å‘é‡è³‡æ–™åº«...")
        
        from src.rag.rag_system import ZiweiRAGSystem
        
        # å‰µå»ºRAGç³»çµ±
        rag_system = ZiweiRAGSystem(logger=logger)
        
        # æª¢æŸ¥å‘é‡åº«ç‹€æ…‹
        stats = rag_system.get_system_status()
        vector_stats = stats.get('vector_store', {})
        initial_docs = vector_stats.get('total_documents', 0)
        
        print(f"ğŸ“Š å‘é‡åº«åˆå§‹ç‹€æ…‹: {initial_docs} æ¢æ–‡æª”")
        
        # æ·»åŠ çŸ¥è­˜åˆ°å‘é‡åº«
        success = rag_system.add_knowledge(knowledge_items)
        
        if success:
            # æª¢æŸ¥æ›´æ–°å¾Œçš„ç‹€æ…‹
            updated_stats = rag_system.get_system_status()
            updated_vector_stats = updated_stats.get('vector_store', {})
            final_docs = updated_vector_stats.get('total_documents', 0)
            
            added_docs = final_docs - initial_docs
            
            print(f"âœ… å‘é‡åº«å°å…¥æˆåŠŸï¼")
            print(f"   æ–°å¢æ–‡æª”: {added_docs}")
            print(f"   ç¸½æ–‡æª”æ•¸: {final_docs}")
            print(f"   å‘é‡åº«è·¯å¾‘: {updated_vector_stats.get('persist_directory', 'unknown')}")
            
            # æ¸¬è©¦æœç´¢
            print("\nğŸ” æ¸¬è©¦æœç´¢åŠŸèƒ½...")
            test_query = "ç´«å¾®æ˜Ÿ"
            search_results = rag_system.search_knowledge(test_query, top_k=3)
            
            print(f"æœç´¢ '{test_query}' æ‰¾åˆ° {len(search_results)} æ¢çµæœ:")
            for i, result in enumerate(search_results[:2], 1):
                content_preview = result['content'][:100] + "..." if len(result['content']) > 100 else result['content']
                print(f"  {i}. {content_preview}")
            
            return True
        else:
            logger.error("å‘é‡åº«å°å…¥å¤±æ•—")
            return False
            
    except Exception as e:
        logger.error(f"PDF å°å…¥éç¨‹å¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="PDF æ–‡ä»¶å°å…¥å‘é‡è³‡æ–™åº«å·¥å…·")
    parser.add_argument('pdf_path', help='PDF æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--chunk-size', type=int, default=1000, help='æ–‡æœ¬å¡Šå¤§å° (é»˜èª: 1000)')
    parser.add_argument('--overlap', type=int, default=200, help='æ–‡æœ¬å¡Šé‡ç–Šå¤§å° (é»˜èª: 200)')
    
    args = parser.parse_args()
    
    # åŸ·è¡Œå°å…¥
    success = await import_pdf_to_vector_db(
        pdf_path=args.pdf_path,
        chunk_size=args.chunk_size,
        overlap=args.overlap
    )
    
    if success:
        print("\nğŸ‰ PDF å°å…¥å®Œæˆï¼")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥:")
        print("  1. é‹è¡Œ python main.py ä½¿ç”¨å®Œæ•´ç³»çµ±")
        print("  2. é‹è¡Œ python manage_vector_db.py status æŸ¥çœ‹å‘é‡åº«ç‹€æ…‹")
        print("  3. é‹è¡Œ python manage_vector_db.py search --query 'é—œéµè©' æ¸¬è©¦æœç´¢")
    else:
        print("\nâŒ PDF å°å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤ä¿¡æ¯")

if __name__ == "__main__":
    print("ğŸ“š ç´«å¾®æ–—æ•¸PDFå°å…¥å·¥å…·")
    print("è«‹ç¢ºä¿å·²å®‰è£PDFè™•ç†åº«:")
    print("  pip install PyPDF2")
    print("  æˆ–")
    print("  pip install pdfplumber")
    print()
    
    asyncio.run(main())
