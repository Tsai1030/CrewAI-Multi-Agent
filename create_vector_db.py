"""
ç°¡å–®çš„å‘é‡è³‡æ–™åº«å»ºç«‹ç¨‹å¼
ä½¿ç”¨ BGE-M3 è™•ç†ç´«å¾®æ–—æ•¸PDFæ–‡ä»¶ï¼Œå»ºç«‹æŒä¹…åŒ–å‘é‡åº«
"""

import os
import logging
from pathlib import Path
from typing import List, Dict, Any
import PyPDF2
import chromadb
from chromadb.config import Settings

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def extract_pdf_content(pdf_path: str) -> str:
    """æå–PDFå…§å®¹"""
    logger.info(f"é–‹å§‹æå–PDFå…§å®¹: {pdf_path}")
    
    try:
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)
            logger.info(f"PDFç¸½é æ•¸: {total_pages}")
            
            full_text = ""
            for page_num in range(total_pages):
                page = pdf_reader.pages[page_num]
                text = page.extract_text()
                full_text += text + "\n"
                
                if (page_num + 1) % 50 == 0:
                    logger.info(f"å·²è™•ç† {page_num + 1}/{total_pages} é ")
            
            logger.info(f"PDFå…§å®¹æå–å®Œæˆï¼Œç¸½å­—æ•¸: {len(full_text)}")
            return full_text
            
    except Exception as e:
        logger.error(f"PDFæå–å¤±æ•—: {str(e)}")
        raise

def analyze_content_structure(text: str) -> Dict[str, Any]:
    """åˆ†æå…§å®¹çµæ§‹"""
    logger.info("åˆ†æPDFå…§å®¹çµæ§‹...")
    
    # æª¢æŸ¥å¸¸è¦‹çš„ç´«å¾®æ–—æ•¸é—œéµè©
    keywords = {
        'ä¸»æ˜Ÿ': ['ç´«å¾®æ˜Ÿ', 'å¤©æ©Ÿæ˜Ÿ', 'å¤ªé™½æ˜Ÿ', 'æ­¦æ›²æ˜Ÿ', 'å¤©åŒæ˜Ÿ', 'å»‰è²æ˜Ÿ', 'å¤©åºœæ˜Ÿ', 'å¤ªé™°æ˜Ÿ', 'è²ªç‹¼æ˜Ÿ', 'å·¨é–€æ˜Ÿ', 'å¤©ç›¸æ˜Ÿ', 'å¤©æ¢æ˜Ÿ', 'ä¸ƒæ®ºæ˜Ÿ', 'ç ´è»æ˜Ÿ'],
        'å®®ä½': ['å‘½å®®', 'å…„å¼Ÿå®®', 'å¤«å¦»å®®', 'å­å¥³å®®', 'è²¡å¸›å®®', 'ç–¾å„å®®', 'é·ç§»å®®', 'å¥´åƒ•å®®', 'å®˜ç¥¿å®®', 'ç”°å®…å®®', 'ç¦å¾·å®®', 'çˆ¶æ¯å®®'],
        'è¼”æ˜Ÿ': ['å·¦è¼”', 'å³å¼¼', 'å¤©é­', 'å¤©é‰', 'æ–‡æ˜Œ', 'æ–‡æ›²', 'ç¥¿å­˜', 'å¤©é¦¬'],
        'ç…æ˜Ÿ': ['æ“ç¾Š', 'é™€ç¾…', 'ç«æ˜Ÿ', 'éˆ´æ˜Ÿ', 'åœ°ç©º', 'åœ°åŠ«'],
        'æ ¼å±€': ['æ ¼å±€', 'ä¸‰åˆ', 'å°å®®', 'æœƒç…§', 'åŒå®®'],
        'é‹å‹¢': ['å¤§é™', 'æµå¹´', 'å°é™', 'é‹å‹¢', 'æµæœˆ', 'æµæ—¥']
    }
    
    analysis = {
        'total_length': len(text),
        'keyword_counts': {},
        'estimated_sections': 0
    }
    
    # çµ±è¨ˆé—œéµè©å‡ºç¾æ¬¡æ•¸
    for category, words in keywords.items():
        count = sum(text.count(word) for word in words)
        analysis['keyword_counts'][category] = count
        logger.info(f"{category}ç›¸é—œå…§å®¹: {count} æ¬¡æåŠ")
    
    # ä¼°ç®—ç« ç¯€æ•¸é‡ï¼ˆåŸºæ–¼å¸¸è¦‹åˆ†éš”ç¬¦ï¼‰
    section_markers = text.count('ç¬¬') + text.count('ç« ') + text.count('ç¯€')
    analysis['estimated_sections'] = section_markers
    
    logger.info(f"ä¼°è¨ˆç« ç¯€æ•¸é‡: {section_markers}")
    
    return analysis

def smart_text_chunking(text: str, analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
    """æ™ºèƒ½æ–‡æœ¬åˆ†å¡Š"""
    logger.info("é–‹å§‹æ™ºèƒ½æ–‡æœ¬åˆ†å¡Š...")
    
    # æ ¹æ“šå…§å®¹åˆ†ææ±ºå®šåˆ†å¡Šç­–ç•¥
    total_length = analysis['total_length']
    
    if total_length < 50000:  # çŸ­æ–‡æª”
        chunk_size = 800
        overlap = 100
    elif total_length < 200000:  # ä¸­ç­‰æ–‡æª”
        chunk_size = 1200
        overlap = 200
    else:  # é•·æ–‡æª”
        chunk_size = 1500
        overlap = 300
    
    logger.info(f"é¸æ“‡åˆ†å¡Šç­–ç•¥: å¡Šå¤§å°={chunk_size}, é‡ç–Š={overlap}")
    
    chunks = []
    start = 0
    chunk_id = 1
    
    while start < len(text):
        end = start + chunk_size
        
        # å°‹æ‰¾åˆé©çš„åˆ†å‰²é»
        if end < len(text):
            # å„ªå…ˆåœ¨å¥è™Ÿè™•åˆ†å‰²
            last_period = text.rfind('ã€‚', start, end)
            if last_period > start + chunk_size // 2:
                end = last_period + 1
            else:
                # å…¶æ¬¡åœ¨é€—è™Ÿè™•åˆ†å‰²
                last_comma = text.rfind('ï¼Œ', start, end)
                if last_comma > start + chunk_size // 2:
                    end = last_comma + 1
        
        chunk_text = text[start:end].strip()
        
        if len(chunk_text) > 50:  # åªä¿ç•™æœ‰æ„ç¾©çš„å¡Š
            # ç°¡å–®çš„å…§å®¹åˆ†é¡
            content_type = classify_content(chunk_text)
            
            chunk_data = {
                'content': chunk_text,
                'metadata': {
                    'chunk_id': chunk_id,
                    'start_pos': start,
                    'end_pos': end,
                    'content_type': content_type,
                    'source': 'ç´«å¾®æ–—æ•°é›†æˆå…¨ä¹¦.pdf'
                }
            }
            chunks.append(chunk_data)
            chunk_id += 1
        
        start = end - overlap if end < len(text) else end
        
        if chunk_id % 100 == 0:
            logger.info(f"å·²è™•ç† {chunk_id} å€‹æ–‡æœ¬å¡Š...")
    
    logger.info(f"æ–‡æœ¬åˆ†å¡Šå®Œæˆï¼Œå…± {len(chunks)} å€‹å¡Š")
    return chunks

def classify_content(text: str) -> str:
    """ç°¡å–®çš„å…§å®¹åˆ†é¡"""
    text_lower = text.lower()
    
    # ä¸»æ˜Ÿç›¸é—œ
    main_stars = ['ç´«å¾®æ˜Ÿ', 'å¤©æ©Ÿæ˜Ÿ', 'å¤ªé™½æ˜Ÿ', 'æ­¦æ›²æ˜Ÿ', 'å¤©åŒæ˜Ÿ', 'å»‰è²æ˜Ÿ', 'å¤©åºœæ˜Ÿ', 'å¤ªé™°æ˜Ÿ', 'è²ªç‹¼æ˜Ÿ', 'å·¨é–€æ˜Ÿ', 'å¤©ç›¸æ˜Ÿ', 'å¤©æ¢æ˜Ÿ', 'ä¸ƒæ®ºæ˜Ÿ', 'ç ´è»æ˜Ÿ']
    if any(star in text for star in main_stars):
        return 'ä¸»æ˜Ÿè§£æ'
    
    # å®®ä½ç›¸é—œ
    palaces = ['å‘½å®®', 'å…„å¼Ÿå®®', 'å¤«å¦»å®®', 'å­å¥³å®®', 'è²¡å¸›å®®', 'ç–¾å„å®®', 'é·ç§»å®®', 'å¥´åƒ•å®®', 'å®˜ç¥¿å®®', 'ç”°å®…å®®', 'ç¦å¾·å®®', 'çˆ¶æ¯å®®']
    if any(palace in text for palace in palaces):
        return 'å®®ä½è§£æ'
    
    # æ ¼å±€ç›¸é—œ
    if any(word in text for word in ['æ ¼å±€', 'ä¸‰åˆ', 'å°å®®', 'æœƒç…§']):
        return 'æ ¼å±€åˆ†æ'
    
    # é‹å‹¢ç›¸é—œ
    if any(word in text for word in ['å¤§é™', 'æµå¹´', 'é‹å‹¢', 'æµæœˆ']):
        return 'é‹å‹¢åˆ†æ'
    
    # åŸºç¤ç†è«–
    if any(word in text for word in ['åŸºç¤', 'ç†è«–', 'æ¦‚å¿µ', 'åŸç†']):
        return 'åŸºç¤ç†è«–'
    
    return 'ä¸€èˆ¬å…§å®¹'

def create_vector_database(chunks: List[Dict[str, Any]], db_name: str = "test1"):
    """å»ºç«‹å‘é‡è³‡æ–™åº«"""
    logger.info(f"é–‹å§‹å»ºç«‹å‘é‡è³‡æ–™åº«: {db_name}")
    
    try:
        # è¨­ç½® BGE-M3 åµŒå…¥
        from src.rag.bge_embeddings import BGEM3Embeddings
        
        # å‰µå»ºåµŒå…¥æ¨¡å‹
        embeddings = BGEM3Embeddings(
            model_name="BAAI/bge-m3",
            device="cpu",
            max_length=1024,
            batch_size=8,
            use_fp16=False
        )
        
        logger.info("BGE-M3 åµŒå…¥æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        
        # å‰µå»º ChromaDB å®¢æˆ¶ç«¯
        persist_directory = f"./vector_db_{db_name}"
        client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # å‰µå»ºæˆ–ç²å–é›†åˆ
        collection_name = f"ziwei_knowledge_{db_name}"
        try:
            collection = client.get_collection(collection_name)
            logger.info(f"ä½¿ç”¨ç¾æœ‰é›†åˆ: {collection_name}")
        except:
            collection = client.create_collection(collection_name)
            logger.info(f"å‰µå»ºæ–°é›†åˆ: {collection_name}")
        
        # æ‰¹æ¬¡è™•ç†æ–‡æª”
        batch_size = 50
        total_chunks = len(chunks)
        
        for i in range(0, total_chunks, batch_size):
            batch_chunks = chunks[i:i + batch_size]
            
            # æº–å‚™æ‰¹æ¬¡æ•¸æ“š
            texts = [chunk['content'] for chunk in batch_chunks]
            metadatas = [chunk['metadata'] for chunk in batch_chunks]
            ids = [f"chunk_{chunk['metadata']['chunk_id']}" for chunk in batch_chunks]
            
            # ç”ŸæˆåµŒå…¥
            logger.info(f"è™•ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(total_chunks + batch_size - 1)//batch_size}")
            embeddings_vectors = embeddings.embed_documents(texts)
            
            # æ·»åŠ åˆ°å‘é‡åº«
            collection.add(
                embeddings=embeddings_vectors,
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
            
            logger.info(f"å·²æ·»åŠ  {len(batch_chunks)} å€‹æ–‡æª”åˆ°å‘é‡åº«")
        
        # æª¢æŸ¥æœ€çµ‚ç‹€æ…‹
        final_count = collection.count()
        logger.info(f"å‘é‡è³‡æ–™åº«å»ºç«‹å®Œæˆï¼")
        logger.info(f"è³‡æ–™åº«åç¨±: {db_name}")
        logger.info(f"å­˜å„²è·¯å¾‘: {persist_directory}")
        logger.info(f"é›†åˆåç¨±: {collection_name}")
        logger.info(f"ç¸½æ–‡æª”æ•¸: {final_count}")
        
        # æ¸¬è©¦æœç´¢
        logger.info("æ¸¬è©¦æœç´¢åŠŸèƒ½...")
        test_query = "ç´«å¾®æ˜Ÿçš„ç‰¹è³ª"
        query_embedding = embeddings.embed_query(test_query)
        
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=3
        )
        
        logger.info(f"æœç´¢æ¸¬è©¦æˆåŠŸï¼Œæ‰¾åˆ° {len(results['documents'][0])} æ¢ç›¸é—œçµæœ")
        
        return True
        
    except Exception as e:
        logger.error(f"å‘é‡è³‡æ–™åº«å»ºç«‹å¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•¸"""
    pdf_path = r"C:\Users\user\Desktop\test2\ç´«å¾®æ–—æ•°é›†æˆå…¨ä¹¦.pdf"
    db_name = "test1"
    
    print("ğŸŒŸ ç´«å¾®æ–—æ•¸å‘é‡è³‡æ–™åº«å»ºç«‹ç¨‹å¼")
    print(f"ğŸ“ PDFæ–‡ä»¶: {pdf_path}")
    print(f"ğŸ—„ï¸ è³‡æ–™åº«åç¨±: {db_name}")
    print("=" * 60)
    
    try:
        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(pdf_path).exists():
            logger.error(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
            return
        
        # 1. æå–PDFå…§å®¹
        text = extract_pdf_content(pdf_path)
        
        # 2. åˆ†æå…§å®¹çµæ§‹
        analysis = analyze_content_structure(text)
        
        # 3. æ™ºèƒ½åˆ†å¡Š
        chunks = smart_text_chunking(text, analysis)
        
        # 4. å»ºç«‹å‘é‡è³‡æ–™åº«
        success = create_vector_database(chunks, db_name)
        
        if success:
            print("\nğŸ‰ å‘é‡è³‡æ–™åº«å»ºç«‹æˆåŠŸï¼")
            print(f"ğŸ“ ä½ç½®: ./vector_db_{db_name}")
            print(f"ğŸ“Š æ–‡æª”æ•¸: {len(chunks)}")
            print("\nâœ… å¯ä»¥é–‹å§‹ä½¿ç”¨å‘é‡è³‡æ–™åº«é€²è¡Œæœç´¢äº†ï¼")
        else:
            print("\nâŒ å‘é‡è³‡æ–™åº«å»ºç«‹å¤±æ•—")
            
    except Exception as e:
        logger.error(f"ç¨‹å¼åŸ·è¡Œå¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
